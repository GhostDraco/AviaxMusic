import requests
import re
from urllib.parse import urljoin, urlparse
from pyrogram import filters
from AviaxMusic import app
from bs4 import BeautifulSoup
import json

__HELP__ = """
** ü…™…¥·¥ã ·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ·¥è Ä ·¥ç·¥è·¥Ö·¥ú ü·¥á**

·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ ·¥Ä ü ü  ü…™…¥·¥ãs ·¥Ä…¥·¥Ö ·¥Ä·¥ò…™ ·¥á…¥·¥Ö·¥ò·¥è…™…¥·¥õs “ì Ä·¥è·¥ç ·¥Ä ·¥°·¥á ôs…™·¥õ·¥á.

**·¥Ñ·¥è·¥ç·¥ç·¥Ä…¥·¥Ös:**
- `/extract <url>` - ·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ ·¥Ä ü ü  ü…™…¥·¥ãs ·¥Ä…¥·¥Ö ·¥Ä·¥ò…™ ·¥á…¥·¥Ö·¥ò·¥è…™…¥·¥õs
- `/extract <url> link` - ·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ ·¥è…¥ ü è  Ä·¥á…¢·¥ú ü·¥Ä Ä  ü…™…¥·¥ãs
- `/extract <url> api` - ·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ ·¥è…¥ ü è ·¥Ä·¥ò…™ ·¥á…¥·¥Ö·¥ò·¥è…™…¥·¥õs

**·¥áx·¥Ä·¥ç·¥ò ü·¥ás:**
- `/extract https://example.com`
- `/extract example.com api`
- `/extract https://api.example.com both`

**·¥ò·¥Ä Ä·¥Ä·¥ç·¥á·¥õ·¥á Äs:**
- `url`: ·¥°·¥á ôs…™·¥õ·¥á URL (·¥°…™·¥õ ú ·¥è Ä ·¥°…™·¥õ ú·¥è·¥ú·¥õ https://)
- `type` (·¥è·¥ò·¥õ…™·¥è…¥·¥Ä ü): `link`, `api`, ·¥è Ä `both` (·¥Ö·¥á“ì·¥Ä·¥ú ü·¥õ)
"""

__MODULE__ = "L…™…¥·¥ãEx·¥õ Ä·¥Ä·¥Ñ·¥õ"

class LinkExtractor:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
    
    def normalize_url(self, url):
        """Add https:// if not present"""
        if not re.match(r'^https?://', url, re.IGNORECASE):
            url = 'https://' + url
        return url
    
    def is_valid_url(self, url):
        """Check if URL is valid"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
    
    def is_api_endpoint(self, url):
        """Check if URL looks like an API endpoint"""
        api_patterns = [
            r'/api/', r'\.php$', r'\.json$', r'\.xml$', 
            r'\.aspx$', r'/v[0-9]+/', r'/endpoint', r'/graphql',
            r'/rest/', r'/soap/', r'/rpc/', r'api\.', r'\.ashx$',
            r'/ajax/', r'/fetch', r'/data', r'/query'
        ]
        
        url_lower = url.lower()
        for pattern in api_patterns:
            if re.search(pattern, url_lower):
                return True
        
        # Check for common API parameter patterns
        api_params = ['api_key=', 'token=', 'access_token=', 'apikey=', 'auth=']
        if any(param in url_lower for param in api_params):
            return True
            
        return False
    
    def extract_links(self, url, extract_type="both"):
        """Main extraction function"""
        try:
            # Normalize URL
            url = self.normalize_url(url)
            
            if not self.is_valid_url(url):
                return {"status": False, "error": "Invalid URL format"}
            
            # Fetch HTML content
            response = self.session.get(url, timeout=10, verify=False)
            response.raise_for_status()
            
            # Parse HTML with BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            all_links = set()
            api_links = set()
            
            # Extract all links
            for tag in soup.find_all(['a', 'link', 'script', 'img', 'iframe', 'form']):
                href = None
                
                if tag.name == 'a' and tag.get('href'):
                    href = tag.get('href')
                elif tag.name == 'link' and tag.get('href'):
                    href = tag.get('href')
                elif tag.name == 'script' and tag.get('src'):
                    href = tag.get('src')
                elif tag.name == 'img' and tag.get('src'):
                    href = tag.get('src')
                elif tag.name == 'iframe' and tag.get('src'):
                    href = tag.get('src')
                elif tag.name == 'form' and tag.get('action'):
                    href = tag.get('action')
                
                if href:
                    # Skip invalid links
                    if href.startswith(('javascript:', 'mailto:', 'tel:', '#')):
                        continue
                    
                    # Make absolute URL
                    absolute_url = urljoin(url, href)
                    
                    # Clean URL
                    absolute_url = absolute_url.split('#')[0].split('?')[0]
                    
                    if self.is_valid_url(absolute_url):
                        all_links.add(absolute_url)
                        
                        # Check if it's an API endpoint
                        if self.is_api_endpoint(absolute_url):
                            api_links.add(absolute_url)
            
            # Also extract from JavaScript and CSS
            js_pattern = r'["\'](https?://[^"\'\s]+)["\']'
            js_links = re.findall(js_pattern, response.text)
            
            for js_link in js_links:
                if self.is_valid_url(js_link):
                    all_links.add(js_link)
                    if self.is_api_endpoint(js_link):
                        api_links.add(js_link)
            
            # Convert sets to sorted lists
            all_links = sorted(list(all_links))
            api_links = sorted(list(api_links))
            
            # Prepare response based on extract_type
            if extract_type == "link":
                data = all_links
            elif extract_type == "api":
                data = api_links
            else:  # both
                data = {
                    "all_links": all_links,
                    "api_links": api_links
                }
            
            return {
                "status": True,
                "host": urlparse(url).netloc,
                "mode": extract_type,
                "total_links": len(all_links),
                "api_links_count": len(api_links),
                "data": data,
                "developer": "@xFlexyy",
                "telegram": "@ScriptFlix_Bots"
            }
            
        except requests.exceptions.RequestException as e:
            return {"status": False, "error": f"Network error: {str(e)}"}
        except Exception as e:
            return {"status": False, "error": f"Extraction error: {str(e)}"}

# Initialize extractor
extractor = LinkExtractor()

@app.on_message(filters.command("extract"))
def extract_links_command(client, message):
    """Handle /extract command"""
    try:
        args = message.text.split()[1:]  # Skip command
        
        if not args:
            message.reply_text(
                "**·¥ús·¥Ä…¢·¥á:**\n"
                "`/extract <url>` - ·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ ·¥Ä ü ü  ü…™…¥·¥ãs\n"
                "`/extract <url> link` - ·¥è…¥ ü è  ü…™…¥·¥ãs\n"
                "`/extract <url> api` - ·¥è…¥ ü è ·¥Ä·¥ò…™ ·¥á…¥·¥Ö·¥ò·¥è…™…¥·¥õs\n"
                "`/extract <url> both` -  ô·¥è·¥õ ú  ü…™…¥·¥ãs & ·¥Ä·¥ò…™s\n\n"
                "**·¥áx·¥Ä·¥ç·¥ò ü·¥á:** `/extract https://example.com api`"
            )
            return
        
        url = args[0]
        extract_type = args[1] if len(args) > 1 else "both"
        
        # Validate extract_type
        if extract_type not in ["link", "api", "both"]:
            extract_type = "both"
        
        # Send processing message
        processing_msg = message.reply_text(f"üîç **·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ…™…¥…¢  ü…™…¥·¥ãs “ì Ä·¥è·¥ç:** `{url}`\n**·¥ç·¥è·¥Ö·¥á:** `{extract_type}`")
        
        # Extract links
        result = extractor.extract_links(url, extract_type)
        
        if not result["status"]:
            processing_msg.edit_text(f"‚ùå **·¥á Ä Ä·¥è Ä:** {result.get('error', 'Unknown error')}")
            return
        
        # Format response
        response_text = f"""
‚úÖ ** ü…™…¥·¥ã ·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ…™·¥è…¥ ·¥Ñ·¥è·¥ç·¥ò ü·¥á·¥õ·¥á**

üåê ** ú·¥ès·¥õ:** `{result['host']}`
üìä **·¥ç·¥è·¥Ö·¥á:** `{result['mode']}`
üîó **·¥õ·¥è·¥õ·¥Ä ü  ü…™…¥·¥ãs:** `{result['total_links']}`
‚ö° **·¥Ä·¥ò…™  ü…™…¥·¥ãs:** `{result['api_links_count']}`
üë®‚Äçüíª **·¥Ö·¥á·¥†·¥á ü·¥è·¥ò·¥á Ä:** {result['developer']}
üì± **·¥õ·¥á ü·¥á…¢ Ä·¥Ä·¥ç:** {result['telegram']}
"""
        
        # Send the summary
        processing_msg.edit_text(response_text)
        
        # Send extracted data in chunks
        if extract_type == "link":
            links = result["data"]
            if links:
                # Send links in batches
                batch_size = 20
                for i in range(0, len(links), batch_size):
                    batch = links[i:i + batch_size]
                    links_text = "**üìÑ ·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ·¥á·¥Ö  ü…™…¥·¥ãs:**\n\n" + "\n".join([f"{j+1}. `{link}`" for j, link in enumerate(batch, i+1)])
                    if i + batch_size < len(links):
                        links_text += f"\n\n... ·¥Ä…¥·¥Ö {len(links) - (i + batch_size)} ·¥ç·¥è Ä·¥á"
                    
                    # Split if message is too long
                    if len(links_text) > 4000:
                        chunks = [links_text[j:j+4000] for j in range(0, len(links_text), 4000)]
                        for chunk in chunks:
                            message.reply_text(chunk)
                    else:
                        message.reply_text(links_text)
            else:
                message.reply_text("‚ùå **…¥·¥è  ü…™…¥·¥ãs “ì·¥è·¥ú…¥·¥Ö**")
        
        elif extract_type == "api":
            apis = result["data"]
            if apis:
                # Send API links in batches
                batch_size = 15
                for i in range(0, len(apis), batch_size):
                    batch = apis[i:i + batch_size]
                    apis_text = "**‚ö° ·¥Ä·¥ò…™ ·¥á…¥·¥Ö·¥ò·¥è…™…¥·¥õs:**\n\n" + "\n".join([f"{j+1}. `{api}`" for j, api in enumerate(batch, i+1)])
                    if i + batch_size < len(apis):
                        apis_text += f"\n\n... ·¥Ä…¥·¥Ö {len(apis) - (i + batch_size)} ·¥ç·¥è Ä·¥á"
                    
                    # Split if message is too long
                    if len(apis_text) > 4000:
                        chunks = [apis_text[j:j+4000] for j in range(0, len(apis_text), 4000)]
                        for chunk in chunks:
                            message.reply_text(chunk)
                    else:
                        message.reply_text(apis_text)
            else:
                message.reply_text("‚ùå **…¥·¥è ·¥Ä·¥ò…™ ·¥á…¥·¥Ö·¥ò·¥è…™…¥·¥õs “ì·¥è·¥ú…¥·¥Ö**")
        
        else:  # both
            # Send all links
            all_links = result["data"]["all_links"]
            api_links = result["data"]["api_links"]
            
            if all_links:
                message.reply_text(f"**üîó ·¥Ä ü ü  ü…™…¥·¥ãs ({len(all_links)}):**\n`{all_links[0]}`\n\n... ·¥Ä…¥·¥Ö {len(all_links)-1} ·¥ç·¥è Ä·¥á")
            
            if api_links:
                message.reply_text(f"**‚ö° ·¥Ä·¥ò…™  ü…™…¥·¥ãs ({len(api_links)}):**\n`{api_links[0]}`\n\n... ·¥Ä…¥·¥Ö {len(api_links)-1} ·¥ç·¥è Ä·¥á")
            
            if not all_links and not api_links:
                message.reply_text("‚ùå **…¥·¥è  ü…™…¥·¥ãs “ì·¥è·¥ú…¥·¥Ö**")
        
        # Send JSON data as file if there are many links
        if result["total_links"] > 30:
            json_data = json.dumps(result, indent=2, ensure_ascii=False)
            message.reply_document(
                document=json_data.encode(),
                file_name=f"extracted_links_{result['host']}.json",
                caption=f"üìÅ **“ì·¥ú ü ü ·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ·¥á·¥Ö ·¥Ö·¥Ä·¥õ·¥Ä**\n\nüåê {result['host']}\nüîó {result['total_links']}  ü…™…¥·¥ãs\n‚ö° {result['api_links_count']} ·¥Ä·¥ò…™s"
            )
    
    except Exception as e:
        message.reply_text(f"‚ùå **·¥á Ä Ä·¥è Ä:** {str(e)}\n\n·¥ò ü·¥á·¥Äs·¥á ·¥Ñ ú·¥á·¥Ñ·¥ã ·¥õ ú·¥á URL ·¥Ä…¥·¥Ö ·¥õ Ä è ·¥Ä…¢·¥Ä…™…¥.")

@app.on_message(filters.command("extractjson"))
def extract_json_command(client, message):
    """Handle /extractjson command to get only JSON output"""
    try:
        args = message.text.split()[1:]
        
        if not args:
            message.reply_text("**·¥ús·¥Ä…¢·¥á:** `/extractjson <url>`")
            return
        
        url = args[0]
        extract_type = args[1] if len(args) > 1 else "both"
        
        processing_msg = message.reply_text(f"üîç **·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ…™…¥…¢ ·¥äs·¥è…¥ ·¥Ö·¥Ä·¥õ·¥Ä “ì Ä·¥è·¥ç:** `{url}`")
        
        result = extractor.extract_links(url, extract_type)
        
        if not result["status"]:
            processing_msg.edit_text(f"‚ùå **·¥á Ä Ä·¥è Ä:** {result.get('error', 'Unknown error')}")
            return
        
        # Send as JSON file
        json_data = json.dumps(result, indent=2, ensure_ascii=False)
        
        processing_msg.delete()
        
        message.reply_document(
            document=json_data.encode(),
            file_name=f"extracted_{result['host']}.json",
            caption=f"üìÅ ** ü…™…¥·¥ã ·¥áx·¥õ Ä·¥Ä·¥Ñ·¥õ…™·¥è…¥  Ä·¥ás·¥ú ü·¥õs**\n\n"
                   f"üåê ** ú·¥ès·¥õ:** {result['host']}\n"
                   f"üìä **·¥ç·¥è·¥Ö·¥á:** {result['mode']}\n"
                   f"üîó **·¥õ·¥è·¥õ·¥Ä ü  ü…™…¥·¥ãs:** {result['total_links']}\n"
                   f"‚ö° **·¥Ä·¥ò…™  ü…™…¥·¥ãs:** {result['api_links_count']}\n"
                   f"üë®‚Äçüíª ** ô è:** {result['developer']}"
        )
    
    except Exception as e:
        message.reply_text(f"‚ùå **·¥á Ä Ä·¥è Ä:** {str(e)}")
